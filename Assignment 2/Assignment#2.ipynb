{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czjcahcys1yO"
   },
   "source": [
    "# Assignment #2\n",
    "\n",
    "# In This assignment you are asked to read a data which include 48505 articles (Documents). Then fint the most similar documents using Locality Sensitive Hashing. Follow the lecture covering this topic step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yjK4_fGbHYIA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import string\n",
    "from scipy.sparse import csr_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frequent_grams = 10000\n",
    "num_hash_functions = 200\n",
    "num_buckets = 10000\n",
    "num_bands = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data is available in Json format and you need to read it. 'https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json' (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tikcro enters into research and license agreem...</td>\n",
       "      <td>Tikcro enters into research and license agreem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Facebook Friend Request Nearly Cost One North ...</td>\n",
       "      <td>A North Carolina woman is trying to warn other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amlin plc UK Regulatory Announcement: Total Vo...</td>\n",
       "      <td>LONDON--(BUSINESS WIRE)--\\n\\nAMLIN plc\\n\\nTOTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Khaleda asks for security</td>\n",
       "      <td>Khaleda asks for security\\n\\n\\n\\nBNP Chairpers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Liberian Health Clinics Reopen Slowly with Ren...</td>\n",
       "      <td>Liberian Health Clinics Reopen Slowly with Ren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              Title  \\\n",
       "0           0  Tikcro enters into research and license agreem...   \n",
       "1           1  Facebook Friend Request Nearly Cost One North ...   \n",
       "2           2  Amlin plc UK Regulatory Announcement: Total Vo...   \n",
       "3           3                          Khaleda asks for security   \n",
       "4           4  Liberian Health Clinics Reopen Slowly with Ren...   \n",
       "\n",
       "                                             Content  \n",
       "0  Tikcro enters into research and license agreem...  \n",
       "1  A North Carolina woman is trying to warn other...  \n",
       "2  LONDON--(BUSINESS WIRE)--\\n\\nAMLIN plc\\n\\nTOTA...  \n",
       "3  Khaleda asks for security\\n\\n\\n\\nBNP Chairpers...  \n",
       "4  Liberian Health Clinics Reopen Slowly with Ren...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Read Data\n",
    "raw_data = pd.read_json('https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json')\n",
    "df = raw_data.copy(deep = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shingle the documents (10 points)\n",
    "### Tips:\n",
    "* Use string package to cleanup the articles e.g, str.maketrans('', '', string.\n",
    "punctuation)\n",
    "* It is better to convert text to lower case that way you get fewer n-grams\n",
    "* apply ngrams(x.split(), n) using ngrams from nltk on the content + title for computing n-grams, for this data n = 2 is suffcient\n",
    "  * You can use n-gram at word level for this task\n",
    "  * try with different n-gram values \n",
    "  * You can use ngrams from nltk for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~”“’'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation + \"”“’\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 2. Shingle the Documents (cleaning)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "number_of_articles = df.shape[0]\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for i in [\"Title\", \"Content\"]:\n",
    "    # Convert to Lowercase and replace newline with space\n",
    "    df[i] = df[i].apply(lambda x: x.lower().replace('\\n', ' '))\n",
    "    # Remove special characters and Numbers\n",
    "    df[i] = df[i].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation + \"”“’0123456789\")))\n",
    "    # Remove stop words so that we don't include them in ngrams\n",
    "    df[i] = df[i].apply(lambda x: ' '.join([w for w in nltk.tokenize.word_tokenize(x) if w not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ngrams\n",
    "df[\"ngrams-2\"] = (df[\"Title\"] + df[\"Content\"]).apply(lambda x: list(ngrams(x.split(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>ngrams-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tikcro enters research license agreement yeda</td>\n",
       "      <td>tikcro enters research license agreement yeda ...</td>\n",
       "      <td>[(tikcro, enters), (enters, research), (resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>facebook friend request nearly cost one north ...</td>\n",
       "      <td>north carolina woman trying warn others facebo...</td>\n",
       "      <td>[(facebook, friend), (friend, request), (reque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>amlin plc uk regulatory announcement total vot...</td>\n",
       "      <td>londonbusiness wire amlin plc total voting rig...</td>\n",
       "      <td>[(amlin, plc), (plc, uk), (uk, regulatory), (r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>khaleda asks security</td>\n",
       "      <td>khaleda asks security bnp chairperson khaleda ...</td>\n",
       "      <td>[(khaleda, asks), (asks, securitykhaleda), (se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>liberian health clinics reopen slowly renewed ...</td>\n",
       "      <td>liberian health clinics reopen slowly renewed ...</td>\n",
       "      <td>[(liberian, health), (health, clinics), (clini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              Title  \\\n",
       "0           0      tikcro enters research license agreement yeda   \n",
       "1           1  facebook friend request nearly cost one north ...   \n",
       "2           2  amlin plc uk regulatory announcement total vot...   \n",
       "3           3                              khaleda asks security   \n",
       "4           4  liberian health clinics reopen slowly renewed ...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  tikcro enters research license agreement yeda ...   \n",
       "1  north carolina woman trying warn others facebo...   \n",
       "2  londonbusiness wire amlin plc total voting rig...   \n",
       "3  khaleda asks security bnp chairperson khaleda ...   \n",
       "4  liberian health clinics reopen slowly renewed ...   \n",
       "\n",
       "                                            ngrams-2  \n",
       "0  [(tikcro, enters), (enters, research), (resear...  \n",
       "1  [(facebook, friend), (friend, request), (reque...  \n",
       "2  [(amlin, plc), (plc, uk), (uk, regulatory), (r...  \n",
       "3  [(khaleda, asks), (asks, securitykhaleda), (se...  \n",
       "4  [(liberian, health), (health, clinics), (clini...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert n-grams into binary vector representation for each document. You can do some optimzations if the matrix is too big. (10 points)\n",
    "* For example,\n",
    "\n",
    "  * Select top 10000 most frequent n-grams.\n",
    "  * You may also try smaller values of n (like 2 or 3) which result in fewer n-grams.\n",
    "  * Finally, you can also try sparse matrix representation. Like csr_matrix from scipy.sparse. It works even with full vocabulary.\n",
    "    * Given a list of n-grams for each document, see how to builid a sparse matrix here https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "coy5Lxd6JcaI"
   },
   "outputs": [],
   "source": [
    "def getFrequentNgrams(articles):\n",
    "    # Your code Here\n",
    "    # Select most frequent n-grams\n",
    "    ngrams = []\n",
    "    for i in articles[\"ngrams-2\"]:\n",
    "        for j in i:\n",
    "            ngrams.append(j)\n",
    "    return [x[0] for x in Counter(ngrams).most_common(num_frequent_grams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "def getBinaryMatrix(docs):\n",
    "    frequent_ngrams = getFrequentNgrams(docs)\n",
    "    # We need to only choose ngrams that are among the frequent ones\n",
    "    frequent_ngrams_as_string_list = []\n",
    "    for i in range(0, len(frequent_ngrams)):\n",
    "        tple = frequent_ngrams[i]\n",
    "        frequent_ngrams_as_string_list.append(f'{tple[0]} {tple[1]}')\n",
    "    all_ngrams_as_strings = docs[\"ngrams-2\"].copy(deep=True).apply(lambda x: [f'{i[0]} {i[1]}' for i in x])\n",
    "    docs = []\n",
    "    for i in all_ngrams_as_strings:\n",
    "        ngrams_in_frequent = []\n",
    "        for j in i:\n",
    "            if j in frequent_ngrams_as_string_list:\n",
    "                ngrams_in_frequent.append(j)\n",
    "        docs.append(ngrams_in_frequent)\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for d in docs:\n",
    "        for term in d:\n",
    "            index = vocabulary.setdefault(term, len(vocabulary))\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    matrix = csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
    "    # I don't know why but I am getting numbers bigger than 1 from csr_matrix\n",
    "    matrix[matrix > 1] = 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinaryMatrix(docs):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            index = vocabulary.setdefault(word, len(vocabulary))\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return csr_matrix((data, indices, indptr), dtype=int).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ngrams = getFrequentNgrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_freq_ngrams = list(map(lambda x: f'{x[0]} {x[1]}', freq_ngrams)) # list of all ngrams, in the format of 'word1 word2'\n",
    "\n",
    "# format the ngrams to be in the format of 'word1 word2'\n",
    "# get ngrams that are in the top 10000 most frequent ngrams\n",
    "docs = df[\"ngrams-2\"].copy(deep=True)\\\n",
    "        .apply(lambda x: [f'{i[0]} {i[1]}' for i in x]) \\\n",
    "        .apply(lambda row: np.intersect1d(row, list_freq_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [research development, st december, today anno...\n",
       "1        [abc news, could cost, daily basis, didnt know...\n",
       "2                            [total number, voting rights]\n",
       "3        [according news, court hearing, former prime, ...\n",
       "4        [blood pressure, didnt even, dont know, dozen ...\n",
       "                               ...                        \n",
       "48500    [available purchase, dozen people, general man...\n",
       "48501    [around world, could use, looks like, making s...\n",
       "48502    [former head, health problems, immediately ava...\n",
       "48503    [country many, daily times, make one, many eve...\n",
       "48504    [authorities said, december pm, early wednesda...\n",
       "Name: ngrams-2, Length: 48505, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_matrix = getBinaryMatrix(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_matrix = binary_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 48505)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(binary_matrix.shape)\n",
    "print(binary_matrix[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. We need hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1. It might be impossible to avoid collisions but as long as the collions are too many it won't matter much. (10 points)\n",
    "* The simplest would be using the builtin hash() function, it can be for example, hash(rownumber) % Numberofbuckets\n",
    "* You can generate several of these hash functions by xoring a random integer (hash(rownumber)^randint) % Numberofbuckets\n",
    "* It can also be a as simple as (rownumber * randint) % Numberofbuckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX7R93sYMoM0"
   },
   "outputs": [],
   "source": [
    "def getHashFunctionValues(numrows, numhashfunctions):\n",
    "    # Your code Here\n",
    "    # return a matrix with hash values\n",
    "    # Each column represents a random permutation of row numbers\n",
    "    hash_matrix = []\n",
    "    for i in range(numrows):\n",
    "        temp = []\n",
    "        for j in range(numhashfunctions):\n",
    "            temp.append(np.random.randint(0, numrows + 1))\n",
    "        hash_matrix.append(temp)\n",
    "    return np.array(hash_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashFunctionValues(numrows, numhashfunctions):\n",
    "    hash_matrix = np.empty((numrows, numhashfunctions))\n",
    "    for i in range(numrows):\n",
    "        r = np.random.randint(2^32-1)\n",
    "        hash_matrix[i] = np.array([hash(x)^r for x in range(numhashfunctions)])\n",
    "    return hash_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_matrix = getHashFunctionValues(binary_matrix.shape[0], num_hash_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_matrix = hash_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 48505)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute minhash following the faster algorithm from the lecture (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix):\n",
    "    # return minhash signature matrix\n",
    "    signature_matrix = np.ones((binary_matrix.shape[1], hash_val_matrix.shape[0])) * np.inf\n",
    "    print(signature_matrix.shape)\n",
    "    for i in range(binary_matrix.shape[1]):\n",
    "        for j in range(hash_val_matrix.shape[0]):\n",
    "            index_comparison = (np.where(binary_matrix[:, i] == 1))[0]\n",
    "            if len(hash_val_matrix[j, index_comparison]) != 0:\n",
    "                signature_matrix[i, j] = min(hash_val_matrix[j, index_comparison])\n",
    "    return signature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix):\n",
    "    signature_matrix = np.ones((hash_val_matrix.shape[0],binary_matrix.shape[1]))*np.inf\n",
    "\n",
    "    for i in np.arange(binary_matrix.shape[0]):\n",
    "        for j in np.where(binary_matrix[i] == 1)[0]:\n",
    "            for h in np.arange(hash_val_matrix.shape[0]):\n",
    "                if signature_matrix[h][j] == np.inf or signature_matrix[h][j] > hash_val_matrix[h][i]:\n",
    "                    signature_matrix[h][j] = hash_val_matrix[h][i]\n",
    "    \n",
    "    return signature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_matrix = getMinHashSignatureMatrix(binary_matrix, hash_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_matrix = signature_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 48505)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.,   7.,  10., ...,   1.,   0.,   3.],\n",
       "       [  3.,   6.,  11., ...,   0.,   1.,   2.],\n",
       "       [  0.,   5.,   8., ...,   3.,   2.,   1.],\n",
       "       ...,\n",
       "       [193., 194., 207., ..., 192., 197., 198.],\n",
       "       [192., 193., 204., ..., 195., 198., 197.],\n",
       "       [193., 192., 205., ..., 194., 199., 196.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hash signature bands into buckets. Find a way to combine all the  signature values in a band and hash them into a number of buckets ususally very high. (10 points)\n",
    "* Easiest way is to add all the signature values in the bucket and use a similar hash function like before\n",
    "* You should use the same hash function for all bands. And all documents ending up in same bucket for at least one band are considered as candidate pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRC-FWUSSAON"
   },
   "outputs": [],
   "source": [
    "def getLSH(signature_matrix, num_bands, num_buckets):\n",
    "    # return lsh buckets or hash table\n",
    "    lsh = {}\n",
    "    \n",
    "    rows_per_band = int(signature_matrix.shape[0] / num_bands)\n",
    "    bands = np.split(signature_matrix, rows_per_band)\n",
    "    \n",
    "    # I don't understand how can I limit number of buckets?\n",
    "    for i in range(len(bands)):\n",
    "        band = bands[i]\n",
    "        for j in range(0, band.shape[1]):\n",
    "            hashed = hash(\"\".join([str(x) for x in band[:, j]]))\n",
    "            if hashed in lsh:\n",
    "                lsh[hashed].append(j)\n",
    "            else:\n",
    "                lsh[hashed] = [j]    \n",
    "    return lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLSH(signature_matrix, num_bands, num_buckets):\n",
    "    lsh = {}\n",
    "    r = int(signature_matrix.shape[0]/num_bands)\n",
    "    for i in range(num_bands):\n",
    "        rand_int = np.random.randint(2^32-1)\n",
    "        for j in range(signature_matrix.shape[1]):\n",
    "            hash_val = hash(tuple(signature_matrix[i*r:(i+1)*r, j]))^rand_int\n",
    "            if hash_val not in lsh:\n",
    "                lsh[hash_val] = set()\n",
    "            lsh[hash_val].add(j)\n",
    "    return lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = getLSH(signature_matrix, num_bands, num_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tune parameters to make sure the threshold is appropriate. (10 points)\n",
    "* plot the probability of two similar items falling in same bucket for different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Choose the best parameters and get nearest neighbors of each articles (20 points)\n",
    "* Jaccard Similarity\n",
    "* convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYqQg7BfVPuN"
   },
   "outputs": [],
   "source": [
    "def getJaccardSimilarityScore(C1, C2):\n",
    "    intersection = len(list(set(C1).intersection(C2)))\n",
    "    union = len(C1) + len(C2)\n",
    "    score = float(intersection / union)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Write the nearest neibhors of each document to submissions.csv (comma separated, first column is the current document followed by a list of nearest neighbors) file and get the score (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6qzcoHoGVasE"
   },
   "outputs": [],
   "source": [
    "# convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket\n",
    "nearest_neighbors = {}\n",
    "for _, idx in lsh.items():\n",
    "    for i in idx:\n",
    "        if i not in nearest_neighbors:\n",
    "            nearest_neighbors[i] = set()\n",
    "        nearest_neighbors[i].update([x for x in idx if x != i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15756/2897229627.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mn_copy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mn_copy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket\n",
    "n_copy = {}\n",
    "for _, idx in lsh.items():\n",
    "    for i in idx:\n",
    "        if i not in n_copy:\n",
    "            n_copy[i] = set()\n",
    "        n_copy[i].update([x for x in idx if x != i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YkEBs1OOWbuZ"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15756/1027196836.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove the neighbors in same buckets but have similarity score < threshold s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mn_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnearest_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msubmission_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msubmission_nid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rehman\\appdata\\local\\programs\\python\\python37\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rehman\\appdata\\local\\programs\\python\\python37\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rehman\\appdata\\local\\programs\\python\\python37\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rehman\\appdata\\local\\programs\\python\\python37\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove the neighbors in same buckets but have similarity score < threshold s\n",
    "submission_id = []\n",
    "submission_nid = []\n",
    "scores = []\n",
    "for article_id, neighbor_ids in n_copy.items():\n",
    "    print(neighbor_ids)\n",
    "    for nid in neighbor_ids:\n",
    "        score = getJaccardSimilarityScore(binary_matrix[:,article_id], binary_matrix[:,nid])\n",
    "        print(score)\n",
    "        if score < s:\n",
    "            nearest_neighbors[article_id].remove(nid) \n",
    "        else:\n",
    "            # add to submission result\n",
    "            submission_id.append(article_id)\n",
    "            submission_nid.append(nid)\n",
    "            scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wivKramXf-1"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['article_id'] = submission_id\n",
    "data['neighbor_id'] = submission_nid\n",
    "data['JaccardScore'] = scores\n",
    "data.sort_values(by=['article_id', 'neighbor_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpqOKfR5Xlqz"
   },
   "outputs": [],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzmz-qQxXpA6"
   },
   "source": [
    "## 10. Write a report + notebook + submission file in a zip file (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
