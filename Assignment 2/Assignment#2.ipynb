{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czjcahcys1yO"
   },
   "source": [
    "# Assignment #2\n",
    "\n",
    "# In This assignment you are asked to read a data which include 48505 articles (Documents). Then fint the most similar documents using Locality Sensitive Hashing. Follow the lecture covering this topic step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yjK4_fGbHYIA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import string\n",
    "from scipy.sparse import csr_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frequent_grams = 10000\n",
    "num_hash_functions = 200\n",
    "num_buckets = 10000\n",
    "num_bands = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ali/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /home/ali/.local/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /home/ali/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ali/.local/lib/python3.10/site-packages (from nltk) (2022.10.31)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data is available in Json format and you need to read it. 'https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json' (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tikcro enters into research and license agreem...</td>\n",
       "      <td>Tikcro enters into research and license agreem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Facebook Friend Request Nearly Cost One North ...</td>\n",
       "      <td>A North Carolina woman is trying to warn other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amlin plc UK Regulatory Announcement: Total Vo...</td>\n",
       "      <td>LONDON--(BUSINESS WIRE)--\\n\\nAMLIN plc\\n\\nTOTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Khaleda asks for security</td>\n",
       "      <td>Khaleda asks for security\\n\\n\\n\\nBNP Chairpers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Liberian Health Clinics Reopen Slowly with Ren...</td>\n",
       "      <td>Liberian Health Clinics Reopen Slowly with Ren...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              Title  \\\n",
       "0           0  Tikcro enters into research and license agreem...   \n",
       "1           1  Facebook Friend Request Nearly Cost One North ...   \n",
       "2           2  Amlin plc UK Regulatory Announcement: Total Vo...   \n",
       "3           3                          Khaleda asks for security   \n",
       "4           4  Liberian Health Clinics Reopen Slowly with Ren...   \n",
       "\n",
       "                                             Content  \n",
       "0  Tikcro enters into research and license agreem...  \n",
       "1  A North Carolina woman is trying to warn other...  \n",
       "2  LONDON--(BUSINESS WIRE)--\\n\\nAMLIN plc\\n\\nTOTA...  \n",
       "3  Khaleda asks for security\\n\\n\\n\\nBNP Chairpers...  \n",
       "4  Liberian Health Clinics Reopen Slowly with Ren...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Read Data\n",
    "raw_data = pd.read_json('https://www.ux.uis.no/~vsetty/data/assignment2_aricles.json')\n",
    "df = raw_data.copy(deep = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index = [i for i in range(15000, 48505)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shingle the documents (10 points)\n",
    "### Tips:\n",
    "* Use string package to cleanup the articles e.g, str.maketrans('', '', string.\n",
    "punctuation)\n",
    "* It is better to convert text to lower case that way you get fewer n-grams\n",
    "* apply ngrams(x.split(), n) using ngrams from nltk on the content + title for computing n-grams, for this data n = 2 is suffcient\n",
    "  * You can use n-gram at word level for this task\n",
    "  * try with different n-gram values \n",
    "  * You can use ngrams from nltk for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~”“’'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation + \"”“’\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rehman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 2. Shingle the Documents (cleaning)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "number_of_articles = df.shape[0]\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for i in [\"Title\", \"Content\"]:\n",
    "    # Convert to Lowercase and replace newline with space\n",
    "    df[i] = df[i].apply(lambda x: x.lower().replace('\\n', ' '))\n",
    "    # Remove special characters and Numbers\n",
    "    df[i] = df[i].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation + \"”“’0123456789\")))\n",
    "    # Remove stop words so that we don't include them in ngrams\n",
    "    df[i] = df[i].apply(lambda x: ' '.join([w for w in nltk.tokenize.word_tokenize(x) if w not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ngrams\n",
    "df[\"ngrams-2\"] = (df[\"Title\"] + df[\"Content\"]).apply(lambda x: list(ngrams(x.split(), 2)))\n",
    "# df[\"ngrams-3\"] = (df[\"Title\"] + df[\"Content\"]).apply(lambda x: list(ngrams(x.split(), 3)))\n",
    "# df[\"ngrams-4\"] = (df[\"Title\"] + df[\"Content\"]).apply(lambda x: list(ngrams(x.split(), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>ngrams-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tikcro enters research license agreement yeda</td>\n",
       "      <td>tikcro enters research license agreement yeda ...</td>\n",
       "      <td>[(tikcro, enters), (enters, research), (resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>facebook friend request nearly cost one north ...</td>\n",
       "      <td>north carolina woman trying warn others facebo...</td>\n",
       "      <td>[(facebook, friend), (friend, request), (reque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>amlin plc uk regulatory announcement total vot...</td>\n",
       "      <td>londonbusiness wire amlin plc total voting rig...</td>\n",
       "      <td>[(amlin, plc), (plc, uk), (uk, regulatory), (r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>khaleda asks security</td>\n",
       "      <td>khaleda asks security bnp chairperson khaleda ...</td>\n",
       "      <td>[(khaleda, asks), (asks, securitykhaleda), (se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>liberian health clinics reopen slowly renewed ...</td>\n",
       "      <td>liberian health clinics reopen slowly renewed ...</td>\n",
       "      <td>[(liberian, health), (health, clinics), (clini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              Title  \\\n",
       "0           0      tikcro enters research license agreement yeda   \n",
       "1           1  facebook friend request nearly cost one north ...   \n",
       "2           2  amlin plc uk regulatory announcement total vot...   \n",
       "3           3                              khaleda asks security   \n",
       "4           4  liberian health clinics reopen slowly renewed ...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  tikcro enters research license agreement yeda ...   \n",
       "1  north carolina woman trying warn others facebo...   \n",
       "2  londonbusiness wire amlin plc total voting rig...   \n",
       "3  khaleda asks security bnp chairperson khaleda ...   \n",
       "4  liberian health clinics reopen slowly renewed ...   \n",
       "\n",
       "                                            ngrams-2  \n",
       "0  [(tikcro, enters), (enters, research), (resear...  \n",
       "1  [(facebook, friend), (friend, request), (reque...  \n",
       "2  [(amlin, plc), (plc, uk), (uk, regulatory), (r...  \n",
       "3  [(khaleda, asks), (asks, securitykhaleda), (se...  \n",
       "4  [(liberian, health), (health, clinics), (clini...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert n-grams into binary vector representation for each document. You can do some optimzations if the matrix is too big. (10 points)\n",
    "* For example,\n",
    "\n",
    "  * Select top 10000 most frequent n-grams.\n",
    "  * You may also try smaller values of n (like 2 or 3) which result in fewer n-grams.\n",
    "  * Finally, you can also try sparse matrix representation. Like csr_matrix from scipy.sparse. It works even with full vocabulary.\n",
    "    * Given a list of n-grams for each document, see how to builid a sparse matrix here https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "coy5Lxd6JcaI"
   },
   "outputs": [],
   "source": [
    "def getFrequentNgrams(articles):\n",
    "    # Your code Here\n",
    "    # Select most frequent n-grams\n",
    "    ngrams = []\n",
    "    for i in articles[\"ngrams-2\"]:\n",
    "        for j in i:\n",
    "            string_ngram = f'{j[0]} {j[1]}'\n",
    "            ngrams.append(string_ngram)\n",
    "    return [x[0] for x in Counter(ngrams).most_common(num_frequent_grams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "def getBinaryMatrix(docs):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for doc in docs:\n",
    "        for word in doc:\n",
    "            index = vocabulary.setdefault(word, len(vocabulary))\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return csr_matrix((data, indices, indptr), dtype=int).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_ngrams = getFrequentNgrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new years',\n",
       " 'new year',\n",
       " 'new york',\n",
       " 'united states',\n",
       " 'years eve',\n",
       " 'prime minister',\n",
       " 'per cent',\n",
       " 'last year',\n",
       " 'airasia flight',\n",
       " 'flight qz',\n",
       " 'associated press',\n",
       " 'bay area',\n",
       " 'search rescue',\n",
       " 'san francisco',\n",
       " 'let us',\n",
       " 'years ago',\n",
       " 'security council',\n",
       " 'said statement',\n",
       " 'high school',\n",
       " 'officials said',\n",
       " 'bad weather',\n",
       " 'java sea',\n",
       " 'next year',\n",
       " 'first time',\n",
       " 'last week',\n",
       " 'family members',\n",
       " 'air force',\n",
       " 'email address',\n",
       " 'human rights',\n",
       " 'said wednesday',\n",
       " 'police said',\n",
       " 'social media',\n",
       " 'two years',\n",
       " 'pangkalan bun',\n",
       " 'local news',\n",
       " 'across country',\n",
       " 'united nations',\n",
       " 'page click',\n",
       " 'islamic state',\n",
       " 'three years',\n",
       " 'oil prices',\n",
       " 'south korea',\n",
       " 'law enforcement',\n",
       " 'posted onby',\n",
       " 'police officers',\n",
       " 'supreme court',\n",
       " 'rescue agency',\n",
       " 'fairfield county',\n",
       " 'rights reserved',\n",
       " 'air traffic',\n",
       " 'missing airasia',\n",
       " 'cicsac cicsac',\n",
       " 'market research',\n",
       " 'saudi arabia',\n",
       " 'news agency',\n",
       " 'years day',\n",
       " 'return home',\n",
       " 'around world',\n",
       " 'airasia plane',\n",
       " 'hong kong',\n",
       " 'health care',\n",
       " 'police department',\n",
       " 'five years',\n",
       " 'los angeles',\n",
       " 'sierra leone',\n",
       " 'us help',\n",
       " 'chief executive',\n",
       " 'many people',\n",
       " 'said would',\n",
       " 'said tuesday',\n",
       " 'news local',\n",
       " 'go back',\n",
       " 'middle east',\n",
       " 'earlier year',\n",
       " 'loved ones',\n",
       " 'last month',\n",
       " 'natural gas',\n",
       " 'york city',\n",
       " 'earlier month',\n",
       " 'vice president',\n",
       " 'need know',\n",
       " 'want go',\n",
       " 'display ad',\n",
       " 'stock quotes',\n",
       " 'privacy policy',\n",
       " 'houston chronicle',\n",
       " 'even though',\n",
       " 'life jacket',\n",
       " 'white house',\n",
       " 'one place',\n",
       " 'un security',\n",
       " 'took place',\n",
       " 'ap —',\n",
       " 'host comics',\n",
       " 'comics enjoy',\n",
       " 'passengers crew',\n",
       " 'wednesday december',\n",
       " 'bodies recovered',\n",
       " 'sheriffs office',\n",
       " 'six months',\n",
       " 'las vegas',\n",
       " 'share story',\n",
       " 'every day',\n",
       " 'county sheriffs',\n",
       " 'west bank',\n",
       " 'real estate',\n",
       " 'health news',\n",
       " 'told reporters',\n",
       " 'people board',\n",
       " 'dont know',\n",
       " 'weve got',\n",
       " 'h latest',\n",
       " 'latest health',\n",
       " 'malaysia airlines',\n",
       " 'everything need',\n",
       " 'information provided',\n",
       " 'longer available',\n",
       " 'pm est',\n",
       " 'december pm',\n",
       " 'official said',\n",
       " 'also said',\n",
       " 'home page',\n",
       " 'least people',\n",
       " 'north korea',\n",
       " 'would like',\n",
       " 'said adding',\n",
       " 'police officer',\n",
       " 'oil gas',\n",
       " 'make sure',\n",
       " 'sorry inconvenience',\n",
       " 'police chief',\n",
       " 'print edition',\n",
       " 'million people',\n",
       " 'federal government',\n",
       " 'read share',\n",
       " 'two days',\n",
       " 'black boxes',\n",
       " 'executive director',\n",
       " 'latest headlines',\n",
       " 'airlines flight',\n",
       " 'shot killed',\n",
       " 'police say',\n",
       " 'wednesday morning',\n",
       " 'agency said',\n",
       " 'one person',\n",
       " 'president barack',\n",
       " 'facebook page',\n",
       " 'previous page',\n",
       " 'business community',\n",
       " 'another story',\n",
       " 'francisco bay',\n",
       " 'barack obama',\n",
       " 'last years',\n",
       " 'climate change',\n",
       " 'dec dec',\n",
       " 'latest news',\n",
       " 'khaleej times',\n",
       " 'also called',\n",
       " 'back previous',\n",
       " 'arts entertainment',\n",
       " 'comprehensive guide',\n",
       " 'inconvenience caused',\n",
       " 'times square',\n",
       " 'league baseball',\n",
       " 'people killed',\n",
       " 'error information',\n",
       " 'provided error',\n",
       " 'error story',\n",
       " 'story requesting',\n",
       " 'requesting longer',\n",
       " 'available either',\n",
       " 'either expired',\n",
       " 'expired pulled',\n",
       " 'pulled merged',\n",
       " 'merged another',\n",
       " 'story sorry',\n",
       " 'click prefer',\n",
       " 'prefer return',\n",
       " 'recent years',\n",
       " 'public authority',\n",
       " 'search area',\n",
       " 'science technology',\n",
       " 'parent company',\n",
       " 'news world',\n",
       " 'also found',\n",
       " 'news views',\n",
       " 'happy new',\n",
       " 'crude oil',\n",
       " 'news features',\n",
       " 'press conference',\n",
       " 'new zealand',\n",
       " 'onestop shop',\n",
       " 'west africa',\n",
       " 'public health',\n",
       " 'state department',\n",
       " 'four years',\n",
       " 'one main',\n",
       " 'current issues',\n",
       " 'area business',\n",
       " 'surabaya airport',\n",
       " 'surabaya indonesia',\n",
       " 'opinion pages',\n",
       " 'presidential elections',\n",
       " 'backstories insights',\n",
       " 'insights unofficial',\n",
       " 'unofficial musings',\n",
       " 'musings chronicles',\n",
       " 'chronicles opinion',\n",
       " 'news discussion',\n",
       " 'discussion current',\n",
       " 'issues chronicle',\n",
       " 'chronicle political',\n",
       " 'political writers',\n",
       " 'shop connecting',\n",
       " 'connecting bay',\n",
       " 'views digital',\n",
       " 'digital frontier',\n",
       " 'frontier news',\n",
       " 'sf gate',\n",
       " 'resource san',\n",
       " 'told afp',\n",
       " 'seven days',\n",
       " 'criminal court',\n",
       " 'security forces',\n",
       " 'war crimes',\n",
       " 'international criminal',\n",
       " 'coast guard',\n",
       " 'bambang soelistyo',\n",
       " 'rs crore',\n",
       " 'went missing',\n",
       " 'read latest',\n",
       " 'take place',\n",
       " 'statement said',\n",
       " 'whats going',\n",
       " 'lost contact',\n",
       " 'oct oct',\n",
       " 'miller said',\n",
       " 'bodies found',\n",
       " 'found dead',\n",
       " 'tony fernandes',\n",
       " 'jan jan',\n",
       " 'tuesday night',\n",
       " 'authorities said',\n",
       " 'weather going',\n",
       " 'de blasio',\n",
       " 'seven bodies',\n",
       " 'letters editor',\n",
       " 'press release',\n",
       " 'domestic violence',\n",
       " 'businesses stock',\n",
       " 'us know',\n",
       " 'whats weather',\n",
       " 'flight mh',\n",
       " 'said one',\n",
       " 'help plan',\n",
       " 'two men',\n",
       " 'local businesses',\n",
       " 'nov nov',\n",
       " 'international airport',\n",
       " 'headlines around',\n",
       " 'two weeks',\n",
       " 'greater danbury',\n",
       " 'aug aug',\n",
       " 'terms conditions',\n",
       " 'stock markets',\n",
       " 'new jersey',\n",
       " 'main reasons',\n",
       " 'high court',\n",
       " 'black box',\n",
       " 'th anniversary',\n",
       " 'st december',\n",
       " 'want send',\n",
       " 'around globe',\n",
       " 'report said',\n",
       " 'sep sep',\n",
       " 'facebook twitter',\n",
       " 'going like',\n",
       " 'may may',\n",
       " 'officials say',\n",
       " 'x z',\n",
       " 'find much',\n",
       " 'flight attendant',\n",
       " 'every morning',\n",
       " 'music scene',\n",
       " 'like tomorrow',\n",
       " 'weather page',\n",
       " 'scores stats',\n",
       " 'said new',\n",
       " 'q r',\n",
       " 'quotes blogs',\n",
       " 'send note',\n",
       " 'find email',\n",
       " 'ups downs',\n",
       " 'death notices',\n",
       " 'tomorrow find',\n",
       " 'much weather',\n",
       " 'sell buy',\n",
       " 'said people',\n",
       " 'days week',\n",
       " 'concealed weapons',\n",
       " 'know favorite',\n",
       " 'world science',\n",
       " 'watch tonight',\n",
       " 'provided associated',\n",
       " 'mar mar',\n",
       " 'reporter editor',\n",
       " 'e read',\n",
       " 'official editorial',\n",
       " 'editorial opinion',\n",
       " 'opinion everything',\n",
       " 'everything presidential',\n",
       " 'local courses',\n",
       " 'courses pga',\n",
       " 'pga keep',\n",
       " 'keep world',\n",
       " 'world golf',\n",
       " 'proud part',\n",
       " 'part hearst',\n",
       " 'hearst corporation',\n",
       " 'corporation family',\n",
       " 'wondering see',\n",
       " 'see weekend',\n",
       " 'weekend wait',\n",
       " 'dvd find',\n",
       " 'n headlines',\n",
       " 'editor columnists',\n",
       " 'get politics',\n",
       " 'politics fix',\n",
       " 'rent check',\n",
       " 'football baseball',\n",
       " 'delivered door',\n",
       " 'technology wondering',\n",
       " 'wondering watch',\n",
       " 'press x',\n",
       " 'forwardlooking statements',\n",
       " 'research reports',\n",
       " 'send us',\n",
       " 'buy car',\n",
       " 'weekly daily',\n",
       " 'traffic control',\n",
       " 'wednesday st',\n",
       " 'need sell',\n",
       " 'year ago',\n",
       " 'every year',\n",
       " 'attorney general',\n",
       " 'new mexico',\n",
       " 'car b',\n",
       " 'daily horoscopes',\n",
       " 'national search',\n",
       " 'jul jul',\n",
       " 'get paid',\n",
       " 'restaurants area',\n",
       " 'general assembly',\n",
       " 'health insurance',\n",
       " 'jun jun',\n",
       " 'enjoy want',\n",
       " 'golf h',\n",
       " 'l read',\n",
       " 'send one',\n",
       " 'funeral notices',\n",
       " 'policy q',\n",
       " 'service schedules',\n",
       " 'schedules columns',\n",
       " 'columns blogs',\n",
       " 'blogs spirituality',\n",
       " 'reviews restaurants',\n",
       " 'w whats',\n",
       " 'celebrate new',\n",
       " 'past years',\n",
       " 'young people',\n",
       " 'next week',\n",
       " 'apr apr',\n",
       " 'buy rent',\n",
       " 'stay informed',\n",
       " 'farley zippy',\n",
       " 'zippy host',\n",
       " 'south korean',\n",
       " 'tuesday morning',\n",
       " 'door every',\n",
       " 'planning zoning',\n",
       " 'local music',\n",
       " 'live around',\n",
       " 'tonight look',\n",
       " 'flight data',\n",
       " 'pope francis',\n",
       " 'vladimir putin',\n",
       " 'note reporter',\n",
       " 'know think',\n",
       " 'keeping home',\n",
       " 'top billboard',\n",
       " 'headlines across',\n",
       " 'opinion pieces',\n",
       " 'places live',\n",
       " 'informed read',\n",
       " 'city council',\n",
       " 'narendra modi',\n",
       " 'editorials offering',\n",
       " 'elections planning',\n",
       " 'zoning votes',\n",
       " 'complete calendar',\n",
       " 'calendar listings',\n",
       " 'listings available',\n",
       " 'available f',\n",
       " 'restaurant reviews',\n",
       " 'plan dinner',\n",
       " 'dinner g',\n",
       " 'home castle',\n",
       " 'castle tips',\n",
       " 'tips keeping',\n",
       " 'tiptop shape',\n",
       " 'j get',\n",
       " 'paid k',\n",
       " 'read send',\n",
       " 'follow ups',\n",
       " 'downs stock',\n",
       " 'wait come',\n",
       " 'come dvd',\n",
       " 'find hottest',\n",
       " 'hottest bands',\n",
       " 'bands local',\n",
       " 'scene top',\n",
       " 'billboard charts',\n",
       " 'charts n',\n",
       " 'pieces one',\n",
       " 'place letters',\n",
       " 'columnists editorials',\n",
       " 'check great',\n",
       " 'great places',\n",
       " 'baseball westerners',\n",
       " 'westerners need',\n",
       " 'morning seven',\n",
       " 'globe provided',\n",
       " 'president obama',\n",
       " 'south africa',\n",
       " 'found along',\n",
       " 'help fill',\n",
       " 'york times',\n",
       " 'family friends',\n",
       " 'washington dc',\n",
       " 'death penalty',\n",
       " 'red cross',\n",
       " 'well read',\n",
       " 'feb feb',\n",
       " 'department said',\n",
       " 'called death',\n",
       " 'three times',\n",
       " 'rewritten redistributed',\n",
       " 'armed forces',\n",
       " 'fill social',\n",
       " 'social calendar',\n",
       " 'calendar complete',\n",
       " 'us note',\n",
       " 'note let',\n",
       " 'read respond',\n",
       " 'respond promise',\n",
       " 'news parent',\n",
       " 'home tiptop',\n",
       " 'markets wondering',\n",
       " 'notices also',\n",
       " 'notices part',\n",
       " 'part announcements',\n",
       " 'announcements section',\n",
       " 'section classifieds',\n",
       " 'classifieds ads',\n",
       " 'ads obits',\n",
       " 'obits also',\n",
       " 'staff written',\n",
       " 'written obituaries',\n",
       " 'editorials p',\n",
       " 'spirituality buy',\n",
       " 'page whos',\n",
       " 'whos walking',\n",
       " 'walking isle',\n",
       " 'isle area',\n",
       " 'crash site',\n",
       " 'told reuters',\n",
       " 'cockpit voice',\n",
       " 'reserved material',\n",
       " 'minimum wage',\n",
       " 'united kingdom',\n",
       " 'published broadcast',\n",
       " 'broadcast rewritten',\n",
       " 'european union',\n",
       " 'bedrooms full',\n",
       " 'full bathrooms',\n",
       " 'said police',\n",
       " 'us news',\n",
       " 'one day',\n",
       " 'one two',\n",
       " 'breast cancer',\n",
       " 'end year',\n",
       " 'said government',\n",
       " 'small business',\n",
       " 'dont think',\n",
       " 'two countries',\n",
       " 'life insurance',\n",
       " 'surabaya singapore',\n",
       " 'education system',\n",
       " 'indonesia singapore',\n",
       " 'palestinian authority',\n",
       " 'please contact',\n",
       " 'economic growth',\n",
       " 'several years',\n",
       " 'past year',\n",
       " 'national security',\n",
       " 'around pm',\n",
       " 'material may',\n",
       " 'palestinian state',\n",
       " 'two bodies',\n",
       " 'task force',\n",
       " 'news stories',\n",
       " 'police reports',\n",
       " 'may published',\n",
       " 'st louis',\n",
       " 'bottom line',\n",
       " 'foreign ministry',\n",
       " 'est updated',\n",
       " 'foreign minister',\n",
       " 'school sports',\n",
       " 'managing director',\n",
       " 'city surabaya',\n",
       " 'wearing life',\n",
       " 'minutes later',\n",
       " 'years old',\n",
       " 'late tuesday',\n",
       " 'sri lanka',\n",
       " 'death toll',\n",
       " 'reports local',\n",
       " 'three months',\n",
       " 'shot dead',\n",
       " 'political parties',\n",
       " 'draft resolution',\n",
       " 'world war',\n",
       " 'move forward',\n",
       " 'state police',\n",
       " 'executive officer',\n",
       " 'business news',\n",
       " 'number people',\n",
       " 'said said',\n",
       " 'east jerusalem',\n",
       " 'minister said',\n",
       " 'worlds largest',\n",
       " 'killed people',\n",
       " 'kuala lumpur',\n",
       " 'san diego',\n",
       " 'two children',\n",
       " 'sources said',\n",
       " 'one year',\n",
       " 'least one',\n",
       " 'distress signal',\n",
       " 'medical center',\n",
       " 'graduations honor',\n",
       " 'palestinian statehood',\n",
       " 'benjamin netanyahu',\n",
       " 'yearold boy',\n",
       " 'israeli occupation',\n",
       " 'office said',\n",
       " 'three children',\n",
       " 'christmas day',\n",
       " 'also known',\n",
       " 'u v',\n",
       " 'things going',\n",
       " 'young man',\n",
       " 'central kalimantan',\n",
       " 'terry rutledge',\n",
       " 'posted wednesday',\n",
       " 'body found',\n",
       " 'could help',\n",
       " 'christmas eve',\n",
       " 'ring new',\n",
       " 'news reviews',\n",
       " 'north america',\n",
       " 'new delhi',\n",
       " 'hazardous waste',\n",
       " 'like us',\n",
       " 'knecht said',\n",
       " 'file photo',\n",
       " 'sexual assault',\n",
       " 'full time',\n",
       " 'police station',\n",
       " 'washington post',\n",
       " 'local education',\n",
       " 'use cookies',\n",
       " 'civil rights',\n",
       " 'heavy rain',\n",
       " 'times union',\n",
       " 'north carolina',\n",
       " 'central bank',\n",
       " 'parts plane',\n",
       " 'jammu kashmir',\n",
       " 'chief minister',\n",
       " 'around us',\n",
       " 'plane crashed',\n",
       " 'would also',\n",
       " 'come back',\n",
       " 'natural resources',\n",
       " 'company said',\n",
       " 'join international',\n",
       " 'first two',\n",
       " 'minister benjamin',\n",
       " 'per day',\n",
       " 'art cummings',\n",
       " 'brian koonz',\n",
       " 'eileen fitzgerald',\n",
       " 'danbury area',\n",
       " 'world health',\n",
       " 'gas prices',\n",
       " 'kootenai county',\n",
       " 'indonesian city',\n",
       " 'washington state',\n",
       " 'eve morning',\n",
       " 'like one',\n",
       " 'us government',\n",
       " 'th century',\n",
       " 'city police',\n",
       " 'press rights',\n",
       " 'caused want',\n",
       " 'pleaded guilty',\n",
       " 'capital region',\n",
       " 'minister narendra',\n",
       " 'president vladimir',\n",
       " 'two people',\n",
       " 'six years',\n",
       " 'k l',\n",
       " 'email addresses',\n",
       " 'years prison',\n",
       " 'according report',\n",
       " 'fireworks display',\n",
       " 'hit water',\n",
       " 'said also',\n",
       " 'months ago',\n",
       " 'district court',\n",
       " 'said two',\n",
       " 'economic development',\n",
       " 'indonesias search',\n",
       " 'international community',\n",
       " 'weather service',\n",
       " 'local government',\n",
       " 'news release',\n",
       " 'news business',\n",
       " 'borneo island',\n",
       " 'southeast texas',\n",
       " 'thousands flocked',\n",
       " 'flocked north',\n",
       " 'north broulee',\n",
       " 'broulee beach',\n",
       " 'beach annual',\n",
       " 'annual sandcastle',\n",
       " 'sandcastle building',\n",
       " 'building competition',\n",
       " 'competition new',\n",
       " 'may also',\n",
       " 'en route',\n",
       " 'civil war',\n",
       " 'buy sell',\n",
       " 'said man',\n",
       " 'morning thousands',\n",
       " 'around country',\n",
       " 'help us',\n",
       " 'us million',\n",
       " 'later said',\n",
       " 'people aboard',\n",
       " 'recovered far',\n",
       " 'weather conditions',\n",
       " 'long time',\n",
       " 'us department',\n",
       " 'dont want',\n",
       " 'foreign policy',\n",
       " 'registered users',\n",
       " 'san franciscos',\n",
       " 'ebola virus',\n",
       " 'fiscal year',\n",
       " 'piedmont avenue',\n",
       " 'new law',\n",
       " 'south carolina',\n",
       " 'obama administration',\n",
       " 'west african',\n",
       " 'veronica rutledge',\n",
       " 'three days',\n",
       " 'coming year',\n",
       " 'financial news',\n",
       " 'would continue',\n",
       " 'secretary state',\n",
       " 'spokesman said',\n",
       " 'several times',\n",
       " 'family home',\n",
       " 'gay lesbian',\n",
       " 'mahmoud abbas',\n",
       " 'presidential candidate',\n",
       " 'two months',\n",
       " 'general election',\n",
       " 'wall street',\n",
       " 'could take',\n",
       " 'copyright associated',\n",
       " 'healthy life',\n",
       " 'looks like',\n",
       " 'find whats',\n",
       " 'agency chief',\n",
       " 'mental health',\n",
       " 'declined comment',\n",
       " 'look world',\n",
       " 'work together',\n",
       " 'private sector',\n",
       " 'one thing',\n",
       " 'percent year',\n",
       " 'world around',\n",
       " 'bird flu',\n",
       " 'government said',\n",
       " 'times menafn',\n",
       " 'news conference',\n",
       " 'daily news',\n",
       " 'writers news',\n",
       " 'market share',\n",
       " 'area search',\n",
       " 'contributed report',\n",
       " 'pm edt',\n",
       " 'wednesday afternoon',\n",
       " 'services business',\n",
       " 'sunday morning',\n",
       " 'fourth street',\n",
       " 'uc berkeley',\n",
       " 'tv radio',\n",
       " 'area check',\n",
       " 'complete coverage',\n",
       " 'orange black',\n",
       " 'strong winds',\n",
       " 'people died',\n",
       " 'grand jury',\n",
       " 'ball drop',\n",
       " 'stock exchange',\n",
       " 'wednesday night',\n",
       " 'r service',\n",
       " 'big one',\n",
       " 'san pablo',\n",
       " 'national league',\n",
       " 'institution since',\n",
       " 'xinhua news',\n",
       " 'bodies including',\n",
       " 'look like',\n",
       " 'edt reads',\n",
       " 'rs per',\n",
       " 'yearold man',\n",
       " 'wednesday dec',\n",
       " 'reporter wednesday',\n",
       " 'wednesday friday',\n",
       " 'across bay',\n",
       " 'lots things',\n",
       " 'crew members',\n",
       " 'made clear',\n",
       " 'norman atlantic',\n",
       " 'cargo ship',\n",
       " 'name email',\n",
       " 'ministry said',\n",
       " 'oil price',\n",
       " 'us president',\n",
       " 'menafn khaleej',\n",
       " 'general manager',\n",
       " 'green everything',\n",
       " 'news opinion',\n",
       " 'politics blog',\n",
       " 'stories one',\n",
       " 'features available',\n",
       " 'photos check',\n",
       " 'see pages',\n",
       " 'movies music',\n",
       " 'look forward',\n",
       " 'kilometers miles',\n",
       " 'boko haram',\n",
       " 'higher education',\n",
       " 'state university',\n",
       " 'american league',\n",
       " 'yellow green',\n",
       " 'address via',\n",
       " 'check big',\n",
       " 'lot offer',\n",
       " 'ave downtown',\n",
       " 'downtown oakland',\n",
       " 'area national',\n",
       " 'manner speaking',\n",
       " 'since home',\n",
       " 'im going',\n",
       " 'north korean',\n",
       " 'killed least',\n",
       " 'civil aviation',\n",
       " 'visit website',\n",
       " 'last two',\n",
       " 'little bit',\n",
       " 'us crude',\n",
       " 'articleplace display',\n",
       " 'ad sfgate',\n",
       " 'sfgate enigmatic',\n",
       " 'enigmatic cityscapes',\n",
       " 'cityscapes paul',\n",
       " 'paul madonna',\n",
       " 'madonna comprehensive',\n",
       " 'guide bay',\n",
       " 'area arts',\n",
       " 'entertainment scene',\n",
       " 'scene bad',\n",
       " 'bad reporter',\n",
       " 'friday american',\n",
       " 'baseball yellow',\n",
       " 'b asmussens',\n",
       " 'asmussens skewed',\n",
       " 'skewed look',\n",
       " 'business city',\n",
       " 'city insider',\n",
       " 'insider local',\n",
       " 'opinion shop',\n",
       " 'shop backstories',\n",
       " 'pages backstories',\n",
       " 'pages politics',\n",
       " 'blog news',\n",
       " 'writers bottom',\n",
       " 'line onestop',\n",
       " 'community onestop',\n",
       " 'community technology',\n",
       " 'technology chronicles',\n",
       " 'chronicles news',\n",
       " 'features topical',\n",
       " 'topical news',\n",
       " 'features arts',\n",
       " 'entertainment lifestyle',\n",
       " 'lifestyle sports',\n",
       " 'sports stock',\n",
       " 'quotes personal',\n",
       " 'personal portfolio',\n",
       " 'portfolio services',\n",
       " 'news c',\n",
       " 'c bear',\n",
       " 'bear news',\n",
       " 'news bear',\n",
       " 'bear fine',\n",
       " 'fine columnist',\n",
       " 'columnist home',\n",
       " 'home plate',\n",
       " 'plate chron',\n",
       " 'chron sunday',\n",
       " 'sunday chronicle',\n",
       " 'chronicle magazine',\n",
       " 'magazine stories',\n",
       " 'reasons classifieds',\n",
       " 'classifieds placing',\n",
       " 'placing ad',\n",
       " 'ad chronicle',\n",
       " 'chronicle carroll',\n",
       " 'carroll garchik',\n",
       " 'garchik johnson',\n",
       " 'johnson nevius',\n",
       " 'nevius farley',\n",
       " 'enjoy information',\n",
       " 'information submitting',\n",
       " 'submitting comments',\n",
       " 'comments features',\n",
       " 'available registered',\n",
       " 'users chronicles',\n",
       " 'chronicles david',\n",
       " 'david einstein',\n",
       " 'einstein boots',\n",
       " 'boots hey',\n",
       " 'hey wrong',\n",
       " 'wrong admit',\n",
       " 'admit sf',\n",
       " 'sf gates',\n",
       " 'gates gossip',\n",
       " 'gossip blog',\n",
       " 'blog fullcolor',\n",
       " 'fullcolor photo',\n",
       " 'photo fiesta',\n",
       " 'fiesta fantastic',\n",
       " 'fantastic footage',\n",
       " 'footage e',\n",
       " 'e email',\n",
       " 'addresses weve',\n",
       " 'got lists',\n",
       " 'lists folks',\n",
       " 'folks chronicle',\n",
       " 'chronicle sf',\n",
       " 'gate change',\n",
       " 'change sf',\n",
       " 'gate user',\n",
       " 'user email',\n",
       " 'via link',\n",
       " 'link news',\n",
       " 'news facts',\n",
       " 'facts advice',\n",
       " 'advice photos',\n",
       " 'one sfs',\n",
       " 'sfs neighbor',\n",
       " 'neighbor across',\n",
       " 'bay lot',\n",
       " 'offer see',\n",
       " 'pages albany',\n",
       " 'albany solano',\n",
       " 'solano ave',\n",
       " 'downtown berkeley',\n",
       " 'berkeley downtown',\n",
       " 'oakland fourth',\n",
       " 'street gourmet',\n",
       " 'gourmet ghetto',\n",
       " 'ghetto grand',\n",
       " 'grand lakeshore',\n",
       " 'lakeshore ave',\n",
       " 'ave jack',\n",
       " 'jack london',\n",
       " 'london square',\n",
       " 'square piedmont',\n",
       " 'avenue rockridge',\n",
       " 'rockridge san',\n",
       " 'pablo corridor',\n",
       " 'corridor telegraph',\n",
       " 'telegraph avenue',\n",
       " 'avenue temescal',\n",
       " 'temescal uc',\n",
       " 'berkeley campus',\n",
       " 'campus westbrae',\n",
       " 'westbrae northbrae',\n",
       " 'northbrae art',\n",
       " 'art books',\n",
       " 'books events',\n",
       " 'events games',\n",
       " 'games gay',\n",
       " 'lesbian guide',\n",
       " 'guide horoscopes',\n",
       " 'horoscopes movies',\n",
       " 'music nightlife',\n",
       " 'nightlife performance',\n",
       " 'performance theater',\n",
       " 'theater dance',\n",
       " 'dance opera',\n",
       " 'opera restaurants',\n",
       " 'restaurants food',\n",
       " 'food search',\n",
       " 'search things',\n",
       " 'things tv',\n",
       " 'radio listings',\n",
       " 'listings plenty',\n",
       " 'plenty bay',\n",
       " 'check comprehensive',\n",
       " 'guide lots',\n",
       " 'going bay',\n",
       " 'area f',\n",
       " 'f complete',\n",
       " 'coverage san',\n",
       " 'francisco ers',\n",
       " 'ers san',\n",
       " 'franciscos animal',\n",
       " 'animal instinct',\n",
       " 'instinct created',\n",
       " 'created phil',\n",
       " 'phil frank',\n",
       " 'frank write',\n",
       " 'write us',\n",
       " 'us listen',\n",
       " 'listen honest',\n",
       " 'honest financial',\n",
       " 'news ers',\n",
       " 'ers raiders',\n",
       " 'raiders scores',\n",
       " 'stats favorite',\n",
       " 'favorite columnists',\n",
       " 'columnists political',\n",
       " 'political cartooning',\n",
       " 'cartooning life',\n",
       " 'life g',\n",
       " 'g personals',\n",
       " 'personals columns',\n",
       " 'columns plant',\n",
       " 'plant garden',\n",
       " 'garden resource',\n",
       " 'baseball orange',\n",
       " 'black nba',\n",
       " 'nba basketball',\n",
       " 'basketball manner',\n",
       " 'speaking fore',\n",
       " 'fore h',\n",
       " 'news diet',\n",
       " 'diet healthy',\n",
       " 'healthy meals',\n",
       " 'meals nutrition',\n",
       " 'nutrition mr',\n",
       " 'mr san',\n",
       " 'francisco history',\n",
       " 'history voice',\n",
       " 'voice westa',\n",
       " 'westa san',\n",
       " 'francisco institution',\n",
       " 'home design',\n",
       " 'design resource',\n",
       " 'area weekly',\n",
       " 'horoscopes minerva',\n",
       " 'minerva chris',\n",
       " 'chris renstrom',\n",
       " 'per hour',\n",
       " 'percent percent',\n",
       " 'victims airasia',\n",
       " 'last night',\n",
       " 'men women',\n",
       " 'peace talks',\n",
       " 'two decades',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only the frequent ngrams we only get 827 cols in binary matrix. So we select all the ngrams and remove the ones that are not frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of all the ngrams of documents remove the ones not in frequent ngrams\n",
    "docs = df[\"ngrams-2\"].copy(deep = True)\n",
    "docs = docs.apply(lambda x: [f'{i[0]} {i[1]}' for i in x])\n",
    "docs = docs.apply(lambda row: np.intersect1d(row, frequent_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "0        [development new, research development, st dec...\n",
      "1        [abc news, could cost, daily basis, didnt know...\n",
      "2                            [total number, voting rights]\n",
      "3        [according news, court hearing, former prime, ...\n",
      "4        [blood pressure, didnt even, dont know, dozen ...\n",
      "                               ...                        \n",
      "14995    [another story, available either, back previou...\n",
      "14996    [cause death, court appearance, court monday, ...\n",
      "14997    [backpacks filled, claimed lives, cold tempera...\n",
      "14998                                   [associated press]\n",
      "14999    [area said, found nothing, gun violence, minut...\n",
      "Name: ngrams-2, Length: 15000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs.txt','w') as file:\n",
    "    for i in docs:\n",
    "        file.write(', '.join(i))\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_copy = []\n",
    "# Read docs from file to save memory\n",
    "with open('docs.txt', 'r') as file:\n",
    "    while True:\n",
    "        line = file.readline().replace('\\n', '')\n",
    "        doc_ngrams = line.split(', ')\n",
    "        docs_copy.append(doc_ngrams)\n",
    "        \n",
    "        if not line:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_matrix = getBinaryMatrix(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking transpose so that the rows are shingles and columns are documents\n",
    "binary_matrix = binary_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 15000)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(binary_matrix.shape)\n",
    "print(binary_matrix[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. We need hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1. It might be impossible to avoid collisions but as long as the collions are too many it won't matter much. (10 points)\n",
    "* The simplest would be using the builtin hash() function, it can be for example, hash(rownumber) % Numberofbuckets\n",
    "* You can generate several of these hash functions by xoring a random integer (hash(rownumber)^randint) % Numberofbuckets\n",
    "* It can also be a as simple as (rownumber * randint) % Numberofbuckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX7R93sYMoM0"
   },
   "outputs": [],
   "source": [
    "# def getHashFunctionValues(numrows, numhashfunctions):\n",
    "#     # Your code Here\n",
    "#     # return a matrix with hash values\n",
    "#     # Each column represents a random permutation of row numbers\n",
    "#     hash_matrix = []\n",
    "#     for i in range(numrows):\n",
    "#         temp = []\n",
    "#         for j in range(numhashfunctions):\n",
    "#             temp.append(np.random.randint(0, numrows + 1))\n",
    "#         hash_matrix.append(temp)\n",
    "#     return np.array(hash_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHashFunctionValues(numrows, numhashfunctions):\n",
    "    # Your code Here\n",
    "    # return a matrix with hash values\n",
    "    hash_matrix = np.empty((numrows, numhashfunctions))\n",
    "    for i in range(numrows):\n",
    "        row = np.empty((1, numhashfunctions))\n",
    "        for j in range(numhashfunctions):\n",
    "            rand_int = np.random.randint(2^32-1)\n",
    "            row[0, j] = hash(i) ^ rand_int % numrows\n",
    "        hash_matrix[i] = row\n",
    "    return hash_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_matrix = getHashFunctionValues(binary_matrix.shape[0], num_hash_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_matrix = hash_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0000e+01, 9.0000e+00, 2.6000e+01, ..., 9.9900e+03, 1.0012e+04,\n",
       "        9.9840e+03],\n",
       "       [2.8000e+01, 2.7000e+01, 2.6000e+01, ..., 1.0001e+04, 1.0009e+04,\n",
       "        9.9980e+03],\n",
       "       [1.8000e+01, 1.8000e+01, 1.3000e+01, ..., 9.9980e+03, 1.0015e+04,\n",
       "        9.9990e+03],\n",
       "       ...,\n",
       "       [2.3000e+01, 2.5000e+01, 1.6000e+01, ..., 1.0006e+04, 9.9860e+03,\n",
       "        1.0011e+04],\n",
       "       [3.0000e+00, 2.0000e+01, 9.0000e+00, ..., 9.9840e+03, 1.0002e+04,\n",
       "        9.9920e+03],\n",
       "       [1.1000e+01, 1.5000e+01, 7.0000e+00, ..., 1.0007e+04, 1.0006e+04,\n",
       "        9.9910e+03]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 15000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute minhash following the faster algorithm from the lecture (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix):\n",
    "#     # return minhash signature matrix\n",
    "#     signature_matrix = np.ones((binary_matrix.shape[1], hash_val_matrix.shape[0])) * np.inf\n",
    "#     print(signature_matrix.shape)\n",
    "#     for i in range(binary_matrix.shape[1]):\n",
    "#         for j in range(hash_val_matrix.shape[0]):\n",
    "#             index_comparison = (np.where(binary_matrix[:, i] == 1))[0]\n",
    "#             if len(hash_val_matrix[j, index_comparison]) != 0:\n",
    "#                 signature_matrix[i, j] = min(hash_val_matrix[j, index_comparison])\n",
    "#     return signature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix):\n",
    "    # Each column represents one document's hash with 200 hash functions\n",
    "    signature_matrix = np.ones((hash_val_matrix.shape[0], binary_matrix.shape[1])) * np.inf\n",
    "\n",
    "    for i in range(binary_matrix.shape[0]):\n",
    "        for j in np.where(binary_matrix[i] == 1)[0]: # for all docs containing the shingle\n",
    "            for k in range(hash_val_matrix.shape[0]):\n",
    "                if signature_matrix[k][j] > hash_val_matrix[k][i]:\n",
    "                    signature_matrix[k][j] = hash_val_matrix[k][i]\n",
    "    \n",
    "    return signature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_matrix = getMinHashSignatureMatrix(binary_matrix, hash_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 15000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.,   1.,   8., ..., 425., 108., 107.],\n",
       "       [  8.,   1.,  25., ..., 434.,  98., 112.],\n",
       "       [  4.,   4.,  16., ..., 419., 123., 127.],\n",
       "       ...,\n",
       "       [  4.,   3.,   4., ..., 419., 126., 118.],\n",
       "       [  3.,   0.,   0., ..., 427., 125., 114.],\n",
       "       [  1.,   0.,  19., ..., 427., 108., 120.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hash signature bands into buckets. Find a way to combine all the  signature values in a band and hash them into a number of buckets ususally very high. (10 points)\n",
    "* Easiest way is to add all the signature values in the bucket and use a similar hash function like before\n",
    "* You should use the same hash function for all bands. And all documents ending up in same bucket for at least one band are considered as candidate pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLSH(signature_matrix, num_bands, num_buckets):\n",
    "    # return lsh buckets or hash table\n",
    "    lsh = {}\n",
    "    rows_per_band = int(signature_matrix.shape[0] / num_bands)\n",
    "    \n",
    "    for i in range(num_bands):\n",
    "        rand_int = np.random.randint(2 ^ 32 - 1)\n",
    "        for j in range(signature_matrix.shape[1]):\n",
    "            # Using the same hash function as before\n",
    "            hash_val = hash(tuple(signature_matrix[i * rows_per_band:(i + 1) * rows_per_band, j])) ^ rand_int\n",
    "            if hash_val not in lsh:\n",
    "                lsh[hash_val] = set()\n",
    "            lsh[hash_val].add(j)\n",
    "    return lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = getLSH(signature_matrix, num_bands, num_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tune parameters to make sure the threshold is appropriate. (10 points)\n",
    "* plot the probability of two similar items falling in same bucket for different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Choose the best parameters and get nearest neighbors of each articles (20 points)\n",
    "* Jaccard Similarity\n",
    "* convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "nYqQg7BfVPuN"
   },
   "outputs": [],
   "source": [
    "# Code from https://www.statology.org/jaccard-similarity-python/\n",
    "def getJaccardSimilarityScore(C1, C2):\n",
    "    intersection = len(list(set(C1).intersection(C2)))\n",
    "    union = (len(C1) + len(C2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Write the nearest neibhors of each document to submissions.csv (comma separated, first column is the current document followed by a list of nearest neighbors) file and get the score (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "6qzcoHoGVasE"
   },
   "outputs": [],
   "source": [
    "# convert hash table into dictionary of article ids and its other articles that hashed in at least 1 same bucket\n",
    "nearest_neighbors = {}\n",
    "for _, idx in lsh.items():\n",
    "    for i in idx:\n",
    "        if i not in nearest_neighbors:\n",
    "            nearest_neighbors[i] = set()\n",
    "        nearest_neighbors[i].update([x for x in idx if x != i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_copy = copy.deepcopy(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkEBs1OOWbuZ"
   },
   "outputs": [],
   "source": [
    "# Remove the neighbors in same buckets but have similarity score < threshold s\n",
    "submission_id = []\n",
    "submission_nid = []\n",
    "scores = []\n",
    "for article_id, neighbor_ids in n_copy.items():\n",
    "    for nid in neighbor_ids:\n",
    "        score = getJaccardSimilarityScore(binary_matrix[:,article_id], binary_matrix[:,nid])\n",
    "        if score < s:\n",
    "            nearest_neighbors[article_id].remove(nid) \n",
    "        else:\n",
    "            # add to submission result\n",
    "            submission_id.append(article_id)\n",
    "            submission_nid.append(nid)\n",
    "            scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wivKramXf-1"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['article_id'] = submission_id\n",
    "data['neighbor_id'] = submission_nid\n",
    "data['jaccard_score'] = scores\n",
    "data.sort_values(by=['article_id', 'neighbor_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpqOKfR5Xlqz"
   },
   "outputs": [],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('submissions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fzmz-qQxXpA6"
   },
   "source": [
    "## 10. Write a report + notebook + submission file in a zip file (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
